'''
    è¿ç”¨Streamlitå®Œæˆå‰ç«¯é¡µé¢æ˜¾ç¤º
'''
import streamlit as st
from VectorBase import VectorStore
from Files import ReadFiles
from LLM import OpenAIChat, ZhipuChat
from Embeddings import M3eBaseEmbedding, BaseEmbeddings
from typing_extensions import List


st.title("ğŸ’¬ RAGå¤§æ¨¡å‹")
st.caption("ğŸš€ èŠå¤©æœºå™¨äººdeveloped by zbz")
if "messages" not in st.session_state:
    st.session_state["messages"] = [{"role": "assistant", "content": "æ‚¨å¥½ï¼Œæœ‰ä»€ä¹ˆå¯ä»¥å¸®æ‚¨?"}]


# æ›´æ–°å‘é‡åº“
@st.cache_resource
def updateVector(_embedding: BaseEmbeddings):
    docs = ReadFiles('./data').get_content(max_token_len=600, cover_content=150)
    print("ReadFiles done \n")
    vector = VectorStore(document = docs)
    print("vector = VectorStore(document = docs) done \n")
    vector.get_vector(EmbeddingModel = _embedding)
    print("vector.get_vector done \n")
    vector.persist(path='C:/Users/zheng/Desktop/bishe/langchain_learning/VecDocData')
    print("vector.persist done \n")
    return vector


# åˆå§‹åŒ–æ¨¡å‹
def init_models():
    # åŠ è½½embedding model
    embedding = M3eBaseEmbedding()
    print("embedding = M3eBaseEmbedding() done \n")
    st.session_state["Embedding_Model"] = embedding
    # åŠ è½½chat model
    chat = ZhipuChat()
    print("chat = ZhipuChat() done \n")
    st.session_state["Chat_Model"] = chat
    # åŠ è½½ç°æœ‰å‘é‡åº“
    vector = VectorStore()
    print("vector = VectorStore() done \n")
    vector.load_vector(path='C:/Users/zheng/Desktop/bishe/langchain_learning/VecDocData')
    print("load_vector done \n")
    st.session_state["vector"] = vector 


# æ£€æŸ¥æ˜¯å¦éœ€è¦åˆå§‹åŒ–æ¨¡å‹
if "Embedding_Model" not in st.session_state:
    with st.spinner("åŠ è½½æ¨¡å‹ing..."):
        init_models()
        print("init_models() done")

def clear_chat_history():
    st.session_state.messages = [{"role": "assistant", "content": "æ‚¨å¥½ï¼Œæœ‰ä»€ä¹ˆå¯ä»¥å¸®æ‚¨?"}]
st.sidebar.button('æ¸…ç©ºèŠå¤©è®°å½•', on_click=clear_chat_history)

def updateVector():
    with st.spinner("æ›´æ–°å‘é‡åº“ing..."):
        if "Embedding_Model" in st.session_state:
            docs = ReadFiles('./data').get_content(max_token_len=600, cover_content=150)
            print("ReadFiles done \n")
            vector = VectorStore(document = docs)
            print("vector = VectorStore(document = docs) done \n")
            vector.get_vector(EmbeddingModel = st.session_state["Embedding_Model"])
            print("vector.get_vector done \n")
            vector.persist(path='C:/Users/zheng/Desktop/bishe/langchain_learning/VecDocData')
            print("vector.persist done \n")
            st.session_state["vector"] = vector
st.sidebar.button('æ›´æ–°å‘é‡åº“', on_click=updateVector)


# æ˜¾ç¤ºå¯¹è¯å†å²
for message in st.session_state["messages"]:
    if message["role"] == "assistant":
        st.chat_message("assistant").write(message["content"])
    else:
        st.chat_message("user").write(message["content"])

# è·å–ç”¨æˆ·è¾“å…¥
user_input = st.chat_input()
if user_input:
    st.chat_message("user").write(user_input)
    contents = st.session_state["vector"].query(
        user_input, 
        EmbeddingModel=st.session_state["Embedding_Model"], 
        k=2
    )
    content = "\n".join(contents)
    response = st.session_state["Chat_Model"].chat(user_input, st.session_state["messages"], content)
    st.session_state["messages"].append({"role": "user", "content": user_input})
    st.session_state["messages"].append({"role": "assistant", "content": response})
    print(f"response : {response}")
    st.rerun()